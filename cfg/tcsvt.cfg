[net]
batch=128
subdivisions=1
height=21
width=21
channels=1
decay=0.0001
clip=0.1

optimizer=sgd
momentum=0.9

# optimizer=adam
# B1=0.9
# B2=0.999
# eps=0.0000001

## Training configurations
learning_rate=0.1
policy=steps
steps=50000,100000,150000
scales=.1,.1,.1
max_batches=200000

## 1st layer: Feature Extraction
[convolutional]
name=conv1
batch_normalize=0
filters=32
size=3
stride=1
pad=1
activation=relu

## 2nd Layer: Depth-wise/ Point-wise + Shrinking
[convolutional]
name=conv2d
batch_normalize=0
size=3
stride=1
pad=1
depthwise=1
activation=linear

[convolutional]
name=conv2p
batch_normalize=0
filters=16
size=1
stride=1
pad=0
activation=relu

## 3rd Layer: Depth-wise/ Point-wise + Shrinking
[convolutional]
name=conv3d
batch_normalize=0
size=3
stride=1
pad=1
depthwise=1
activation=linear

[convolutional]
name=conv3p
batch_normalize=0
filters=32
size=1
stride=1
pad=0
activation=relu

[shortcut]
from=-5
activation=relu

## 4th Layer: Depth-wise/ Point-wise + Shrinking
[convolutional]
name=conv4d
batch_normalize=0
size=3
stride=1
pad=1
depthwise=1
activation=linear

[convolutional]
name=conv4p
batch_normalize=0
filters=16
size=1
stride=1
pad=0
activation=relu

## 5th Layer: Depth-wise/ Point-wise 
[convolutional]
name=conv5d
batch_normalize=0
size=3
stride=1
pad=1
depthwise=1
activation=linear

## 6th Layer: Deconvolution
[convolutional]
name=conv6
batch_normalize=0
filters=4
size=3
stride=1
pad=1
activation=linear

[stack]
root_connect=1
stride=4

[shortcut]
from=-2
activation=linear

[sr_flat]

[cost]
type=sse